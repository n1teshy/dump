{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this file builds up on 0_next_token_generation.txt, please read it if you haven't already\n",
    "import math\n",
    "\n",
    "\n",
    "# the language model spits out logits(not probabilites) for every token\n",
    "# e.g. logits = {\"the\": 1.1, \"quick\": 0.5, \"brown\": 1.2, \"fox\": 0.9}\n",
    "# these logits are then converted to probabilites using softmax\n",
    "# if the model's vocabulary only contains the above 4 tokens\n",
    "# probability of \"the\" = softmax(\"the\") = (e ^ logits[\"the\"]) / sum((e ^ logit) for logit in logits.values())\n",
    "#                                       = 3.0 / 10.4\n",
    "#                                       = 0.287\n",
    "# the model chooses the next token based on these probabilities, the temperature value, evens\n",
    "# these probabilities out or spikes the already relatively larger logits based on its own value.\n",
    "# large temperature = probabilities are flattened(evened out), smaller temp = relatively larger logits\n",
    "# get even more weight, which means the model sticks strictly to what it has learned in its training\n",
    "\n",
    "\n",
    "def softmax(logits: list[float]) -> list[float]:\n",
    "    denom = sum(math.exp(logit) for logit in logits)\n",
    "    return [math.exp(logit)/denom for logit in logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token -> logit -> adjusted logit -> probability\n",
      "the      1.10 -> 1.10 -> 0.21\n",
      "quick    0.50 -> 0.50 -> 0.12\n",
      "brown    0.85 -> 0.85 -> 0.16\n",
      "fox      0.97 -> 0.97 -> 0.18\n",
      "jumped   1.20 -> 1.20 -> 0.23\n",
      "over     0.30 -> 0.30 -> 0.09\n"
     ]
    }
   ],
   "source": [
    "# temperature = 1 doesn't change anything\n",
    "temp = 1\n",
    "tokens = [\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\"]\n",
    "logits = [1.1, 0.5, 0.85, 0.97, 1.2, 0.3]\n",
    "adjusted_logits = [logit / temp for logit in logits]\n",
    "probs = softmax(adjusted_logits)\n",
    "\n",
    "print(\"token -> logit -> adjusted logit -> probability\")\n",
    "for idx in range(len(logits)):\n",
    "    print(f\"{tokens[idx]:<8} {logits[idx]:.2f} -> {adjusted_logits[idx]:.2f} -> {probs[idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token -> logit -> adjusted logit -> probability\n",
      "the      1.10 -> 0.56 -> 0.19\n",
      "quick    0.50 -> 0.26 -> 0.14\n",
      "brown    0.85 -> 0.44 -> 0.17\n",
      "fox      0.97 -> 0.50 -> 0.18\n",
      "jumped   1.20 -> 0.62 -> 0.20\n",
      "over     0.30 -> 0.15 -> 0.13\n"
     ]
    }
   ],
   "source": [
    "# temperature > 1, flattens the probability distribution, so every token gets equal-ish\n",
    "# probability to be the next token, compare with the previous output to see difference.\n",
    "\n",
    "# this simulates creativity, how? since the probabilities are flattened and every token has\n",
    "# equal chance to be next, there are many legitimate next-token choices, there will be a lot of\n",
    "# diversity in the model's output if you generate the next token many times in this way, as opposed\n",
    "# to selecting the max-probality-token without temperature.\n",
    "temp = 1.95\n",
    "logits = [1.1, 0.5, 0.85, 0.97, 1.2, 0.3]\n",
    "adjusted_logits = [logit / temp for logit in logits]\n",
    "probs = softmax(adjusted_logits)\n",
    "\n",
    "print(\"token -> logit -> adjusted logit -> probability\")\n",
    "for idx in range(len(logits)):\n",
    "    print(f\"{tokens[idx]:<8} {logits[idx]:.2f} -> {adjusted_logits[idx]:.2f} -> {probs[idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token-> logit -> adjusted logit -> probability\n",
      "the      1.10 -> 5.50 -> 0.41\n",
      "quick    0.70 -> 3.50 -> 0.06\n",
      "brown    0.85 -> 4.25 -> 0.12\n",
      "fox      0.90 -> 4.50 -> 0.15\n",
      "jumped   1.02 -> 5.10 -> 0.27\n"
     ]
    }
   ],
   "source": [
    "# temperature < 1 spikes the probabilities of already relatively-larger logits\n",
    "# making the model stick to what it has learned during training, the tokens with higher\n",
    "# logits are what the model has learned from the training dataset, so selecting those tokens\n",
    "# means sticking to the training rigidly, you won't get much diversity in generated text in this\n",
    "# way.\n",
    "temp = 0.2\n",
    "logits = [1.1, 0.7, 0.85, 0.9, 1.02]\n",
    "adjusted_logits = [logit / temp for logit in logits]\n",
    "probs = softmax(adjusted_logits)\n",
    "\n",
    "print(\"token-> logit -> adjusted logit -> probability\")\n",
    "for idx in range(len(logits)):\n",
    "    print(f\"{tokens[idx]:<8} {logits[idx]:.2f} -> {adjusted_logits[idx]:.2f} -> {probs[idx]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
